{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c77bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyresparser import ResumeParser\n",
    "import os\n",
    "from docx import Document\n",
    "from sematch.semantic.similarity import WordNetSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "187ea28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Skills = pd.read_csv('skills.csv', header=None)\n",
    "Skills = Skills.T\n",
    "SkillsArray = Skills.to_numpy()\n",
    "SkillsArray = SkillsArray.tolist()\n",
    "empty = []\n",
    "for e in SkillsArray:\n",
    "    empty.append(e[0])\n",
    "SkillsArray = empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0dd30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "with pdfplumber.open(\"cv.pdf\") as pdf:\n",
    "    first_page = pdf.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e72a27dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Commitment': 0, 'Professional': 0, 'Ethical': 0, 'Creative': 0, 'Innovative': 0, 'Handles Stress': 0, 'Self-Aware': 0, 'Communication': 0, 'Conflict Management': 0, 'Negotiation': 0, 'Leadership': 0, 'Team-work': 0, 'Adaptabiility': 0, 'Analytical': 0, 'Decision Making': 0, 'Project Management': 0, 'Results orientation ': 0}\n"
     ]
    }
   ],
   "source": [
    "wns = WordNetSimilarity()\n",
    "\n",
    "def pdfToText(fileLocation, pdf):   \n",
    "    pages = pdf.pages\n",
    "    return pages\n",
    "\n",
    "def wordSim(word1, word2):\n",
    "    wordsim = wns.word_similarity(word1, word2, 'li') \n",
    "    return wordsim \n",
    "\n",
    "def SimilitaryCheck(wordList, skillsArray, simDict):\n",
    "    score = [0]*len(skillsArray)\n",
    "    #print(skillsArray)\n",
    "    wordIndex = 0 \n",
    "    for i in skillsArray:\n",
    "        for e in wordList:\n",
    "            wordsim = wordSim(e, i)\n",
    "            if  wordsim > 0.45:\n",
    "                print(\"{} has a similarity with {} of {}\".format(e,i , wordsim))\n",
    "                simDict[e] = [i, wordsim]\n",
    "                score[wordIndex] += 1\n",
    "        wordIndex += 1 \n",
    "    return dict(zip(skillsArray,score)) \n",
    "\n",
    "#simDict = {}\n",
    "list = ['Savvy', 'Smart', 'Stupid', 'Daft', 'Clean', 'Clever', 'Wise','Articulate']\n",
    "ScoreDictionary = SimilitaryCheck(list, SkillsArray, simDict)\n",
    "print(ScoreDictionary)\n",
    "#print(simDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94a3709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagefilter(pages):\n",
    "    extractedWords = []\n",
    "    for p in pages:\n",
    "        pageWords = p.extract_words(x_tolerance=2, y_tolerance=3, keep_blank_chars=False, use_text_flow=False, horizontal_ltr=True, vertical_ttb=True)\n",
    "        for w in pageWords:\n",
    "            extractedWords.append(w['text'])\n",
    "    return extractedWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9e7d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileLocation = 'cv.pdf'\n",
    "with pdfplumber.open(fileLocation) as pdf:\n",
    "    pages = pdfToText(fileLocation, pdf)\n",
    "    filtered = pagefilter(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164d6fd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10488/808290660.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtopWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mfiltered_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mfilteredBigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk import SnowballStemmer \n",
    "from nltk import corpus\n",
    "from nltk import bigrams, trigrams, ngrams\n",
    "import re \n",
    "\n",
    "def test(filtered, topN):\n",
    "    sbst=SnowballStemmer('english') \n",
    "    fdist = FreqDist()\n",
    "    stemmed = []\n",
    "    post_punctuation = []\n",
    "    punctuation=re.compile(r'[-.?!,:;()|0-9]')\n",
    "    for word in filtered:\n",
    "        word=punctuation.sub(\"\",word)\n",
    "        if len(word)>0:\n",
    "             post_punctuation.append(word)\n",
    "    filteredPostPunc = [word for word in post_punctuation if word not in corpus.stopwords.words('english')]\n",
    "    for word in filteredPostPunc:\n",
    "        if len(word) > 4:\n",
    "            fdist[word.lower()]+=1\n",
    "    fdist_topWords = fdist.most_common(topN)\n",
    "    topWords = []\n",
    "    for t in fdist_topWords:\n",
    "        topWords.append(t[0])\n",
    "    return topWords\n",
    "\n",
    "topWords = test(filtered, 25)\n",
    "filtered_bigrams = bigrams(filtered)\n",
    "filteredBigrams = []\n",
    "for f in filtered_bigrams:\n",
    "    #print(f[0]+\" \"+f[1])\n",
    "    filteredBigrams.append(f[0]+\" \"+f[1])\n",
    "simDict = {}\n",
    "scoreDict = SimilitaryCheck(topWords, SkillsArray, simDict)\n",
    "topBigrams = test(filteredBigrams, 25)\n",
    "print(\"top Words: --------------------------------------\")\n",
    "print(topWords)\n",
    "print(\"top Bigrams: --------------------------------------\")\n",
    "print(topBigrams)\n",
    "#scoreBigramsDict = SimilitaryCheck(filteredBigrams, SkillsArray)\n",
    "print(\"Word scores: --------------------------------------\")\n",
    "print(scoreDict)\n",
    "#print(\"Bigram Scores: -------------------------------------\")\n",
    "#print(scoreBigramsDict)\n",
    "print(\"similarityDic ------------------------------\")\n",
    "print(simDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96d94469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ahmed m\n",
      "m abdelmoniem\n",
      "cid ahmed\n",
      "conference on\n",
      "brahim bensaou\n",
      "proceedings of\n",
      "international conference\n",
      "computer science\n",
      "science and\n",
      "in data\n",
      "abdelmoniem and\n",
      "marco canini\n",
      "and brahim\n",
      "of ieee\n",
      "of the\n",
      "ieee international\n",
      "machine learning\n",
      "data centers\n",
      "congestion control\n",
      " kaust\n",
      "data centers‚Äù\n",
      "and engineering\n",
      "abdelmoniem brahim\n",
      "control for\n",
      "computer networks\n"
     ]
    }
   ],
   "source": [
    "\"\"\" #Tried to avg simialirites to get bigram but it didn't work :( )\n",
    "def getSim(word, simDict):\n",
    "    sim = simDict.get(word)\n",
    "    if sim != None:\n",
    "        return sim\n",
    "    return 0 \n",
    "    \n",
    "def average(sim1, sim2):\n",
    "    total = sim1 + sim2\n",
    "    avg = total/2\n",
    "    return avg\n",
    "    \n",
    "for bi in topBigrams:\n",
    "    bigramBag = (bi.split(\" \"))\n",
    "    fWordSim = getSim(bigramBag[0], simDict)\n",
    "    sWordSim = getSim(bigramBag[1], simDict)\n",
    "    #print(\"{}, has a sim of {}\".format(bigramBag[0], fWordSim))\n",
    "    #print(\"{}, has a sim of {}\".format(bigramBag[1], sWordSim))\n",
    "    avg = average(fWordSim, sWordSim)\n",
    "    print(\"{} {} has similarity of \".format(bigramBag[0],bigramBag[1]))\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df6ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
